{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogLeNet。 这对于传统的机器学习分类算法而言，已经相当的出色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首次在CNN中成功应用了ReLU、Dropout和LRN等Trick。同时AlexNet也使用了GPU进行运算加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet中包含了几个比较新的技术点，也首次在CNN中成功应用了ReLU、Dropout和LRN等Trick。同时AlexNet也使用了GPU进行运算加速。\n",
    "#### AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到了很深很宽的网络中。AlexNet主要使用到的新技术点如下：\n",
    "- 成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。\n",
    "- 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。\n",
    "- 在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。\n",
    "- 提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。\n",
    "- 使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。 \n",
    "- 数据增强，随机地从256*256的原始图像中截取224*224大小的区域（以及水平翻转的镜像），相当于增加了2*(256-224)^2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。 [1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化\n",
    "##### 使用ReLU 后，会发现激活函数之后的值没有了tanh、sigmoid函数那样有一个值域区间，所以一般在ReLU之后会做一个normalization，LRU就是稳重提出一种方法，在神经科学中有个概念叫“Lateral inhibition”，讲的是活跃的神经元对它周边神经元的影响。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os  \n",
    "import random  \n",
    "import cv2  \n",
    "import math  \n",
    "import time  \n",
    "import numpy as np  \n",
    "import tensorflow as tf  \n",
    "import linecache  \n",
    "import string  \n",
    "import skimage  \n",
    "import imageio  \n",
    "# 输入数据  \n",
    "import input_data  \n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)  \n",
    "# 定义网络超参数  \n",
    "learning_rate = 0.001  \n",
    "training_iters = 200000  \n",
    "batch_size = 64  \n",
    "display_step = 20  \n",
    "# 定义网络参数  \n",
    "n_input = 784  # 输入的维度  \n",
    "n_classes = 10 # 标签的维度  \n",
    "dropout = 0.8  # Dropout 的概率  \n",
    "# 占位符输入  \n",
    "x = tf.placeholder(tf.types.float32, [None, n_input])  \n",
    "y = tf.placeholder(tf.types.float32, [None, n_classes])  \n",
    "keep_prob = tf.placeholder(tf.types.float32)  \n",
    "# 卷积操作  \n",
    "def conv2d(name, l_input, w, b):  \n",
    "    return tf.nn.relu(tf.nn.bias_add( \\  \n",
    "    tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b) \\  \n",
    "    , name=name)  \n",
    "# 最大下采样操作  \n",
    "def max_pool(name, l_input, k):  \n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], \\  \n",
    "    strides=[1, k, k, 1], padding='SAME', name=name)  \n",
    "# 归一化操作  \n",
    "def norm(name, l_input, lsize=4):  \n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)  \n",
    "# 定义整个网络   \n",
    "def alex_net(_X, _weights, _biases, _dropout):  \n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1]) # 向量转为矩阵  \n",
    "    # 卷积层  \n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])  \n",
    "    # 下采样层  \n",
    "    pool1 = max_pool('pool1', conv1, k=2)  \n",
    "    # 归一化层  \n",
    "    norm1 = norm('norm1', pool1, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)  \n",
    "   \n",
    "    # 卷积  \n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])  \n",
    "    # 下采样  \n",
    "    pool2 = max_pool('pool2', conv2, k=2)  \n",
    "    # 归一化  \n",
    "    norm2 = norm('norm2', pool2, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)  \n",
    "   \n",
    "    # 卷积  \n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])  \n",
    "    # 下采样  \n",
    "    pool3 = max_pool('pool3', conv3, k=2)  \n",
    "    # 归一化  \n",
    "    norm3 = norm('norm3', pool3, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)  \n",
    "   \n",
    "    # 全连接层，先把特征图转为向量  \n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]])   \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')   \n",
    "    # 全连接层  \n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') \n",
    "    # Relu activation  \n",
    "    # 网络输出层  \n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']  \n",
    "    return out  \n",
    "   \n",
    "# 存储所有的网络参数  \n",
    "weights = {  \n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),  \n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),  \n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),  \n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),  \n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),  \n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))  \n",
    "}  \n",
    "biases = {  \n",
    "    'bc1': tf.Variable(tf.random_normal([64])),  \n",
    "    'bc2': tf.Variable(tf.random_normal([128])),  \n",
    "    'bc3': tf.Variable(tf.random_normal([256])),  \n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),  \n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),  \n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))  \n",
    "}  \n",
    "# 构建模型  \n",
    "pred = alex_net(x, weights, biases, keep_prob)  \n",
    "# 定义损失函数和学习步骤  \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  \n",
    "# 测试网络  \n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
    "# 初始化所有的共享变量  \n",
    "init = tf.initialize_all_variables()  \n",
    "# 开启一个训练  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)  \n",
    "    step = 1  \n",
    "    # Keep training until reach max iterations  \n",
    "    while step * batch_size < training_iters:  \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "        # 获取批数据  \n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})  \n",
    "        if step % display_step == 0:  \n",
    "            # 计算精度  \n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  \n",
    "            # 计算损失值  \n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  \n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)  \n",
    "        step += 1  \n",
    "    print \"Optimization Finished!\"  \n",
    "    # 计算测试精度  \n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Alexnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
